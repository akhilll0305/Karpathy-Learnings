# ğŸš€ LLM Engineering Journey â€“ From Scratch Implementations

## ğŸ“Œ Overview

This repository documents my hands-on journey into understanding **Large Language Models (LLMs)** from first principles.

Instead of relying on high-level libraries blindly, this repo focuses on building core components manually to deeply understand:

- Self-attention
- Transformer blocks
- GPT architecture
- Tokenization
- Training loops
- Loss functions
- Scaling behavior

This learning path is inspired by:

- Andrej Karpathy's educational videos
- The original Transformer paper: *Attention Is All You Need*

---

## ğŸ“‚ Repository Structure

```bash
Karpathy-Learnings/
â”‚
â”œâ”€â”€ README.md
â”‚
â”œâ”€â”€ Build GPT/
â”‚   â”œâ”€â”€ bigram.py
â”‚   â”œâ”€â”€ gpt.py
â”‚   â”œâ”€â”€ input.txt
â”‚   â””â”€â”€ more.txt
â”‚
â””â”€â”€ GPT Tokenizer/   # ğŸš§ In progress
```

---

## ğŸ“– Modules

### 1ï¸âƒ£ `Build GPT/`

Implementation of a GPT-style decoder-only Transformer from scratch in PyTorch.

#### ğŸ“š Covers

- Bigram language model
- Self-attention
- Multi-head attention
- Transformer block
- LayerNorm (Pre-LN vs Post-LN understanding)
- Residual connections
- Causal masking
- Training loop
- Text generation

#### ğŸ§  Key Learning Outcomes

- How autoregressive modeling works
- How masked attention enforces causality
- Why residual connections stabilize training
- Why modern LLMs use Pre-LN
- How logits â†’ softmax â†’ cross-entropy loss works

---

### 2ï¸âƒ£ `tokenizer/` *(Next Phase ğŸš§)*

Planned implementation based on Karpathy's tokenizer deep dive.

#### ğŸ“š Will Cover

- Byte Pair Encoding (BPE)
- Vocabulary construction
- Merge rules
- Encoding & decoding
- Token distribution analysis

#### ğŸ¯ Goal

Understand how raw text becomes model-consumable tokens.

---

## ğŸ§  Core Concepts Understood

### ğŸ”¹ Self-Attention

Tokens dynamically weigh relevance of previous tokens using:

$$\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d}}\right)V$$

### ğŸ”¹ Causal Masking

Prevents a token from attending to future tokens during training and generation.

### ğŸ”¹ Transformer Block

> Communication â†’ Computation â†’ Stabilization

```
LayerNorm
   â†“
Self-Attention
   â†“
Residual Add
   â†“
LayerNorm
   â†“
FeedForward
   â†“
Residual Add
```

### ğŸ”¹ Decoder-Only Architecture

Used in GPT-style models:

- Masked self-attention
- No encoder
- Autoregressive generation
- Next-token prediction objective

---

## ğŸ”¬ Future Additions

- [ ] Tokenizer implementation
- [ ] FlashAttention exploration
- [ ] RoPE positional embeddings
- [ ] Multi-Query Attention
- [ ] Scaling experiments
- [ ] Small-scale pretraining experiments

---

## ğŸ“ˆ Why This Repository Exists

This repository is not just code â€” it is a structured learning archive.

The goal is to:

- Move from **user** of LLMs â†’ **builder** of LLMs
- Understand architectural evolution
- Build intuition for modern systems
- Develop implementation-level clarity

---

## ğŸ“Œ Long-Term Vision

Build towards:

- Training small LLMs from scratch
- Understanding modern improvements
- Evaluating model behavior
- Implementing research papers

---

## ğŸ›  Tech Stack

| Tool | Purpose |
|------|---------|
| Python | Primary language |
| PyTorch | Deep learning framework |
| Minimal external deps | Keep it from scratch |

---

## âœï¸ Author Notes

This repository is continuously evolving as I explore deeper into LLM architecture, optimization techniques, and tokenizer design.